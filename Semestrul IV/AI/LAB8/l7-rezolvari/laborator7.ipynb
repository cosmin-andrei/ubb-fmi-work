{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T07:01:07.830355Z",
     "start_time": "2024-04-22T07:01:07.824913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ],
   "id": "71c4d9e9cd72f2d6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T07:29:17.638760Z",
     "start_time": "2024-04-22T07:29:17.622279Z"
    }
   },
   "source": [
    "# 1a) Generare în limba română: Implementați un sistem care transformă un text (corpus) într-un lanț Markov și folosiți-l pentru a generare un proverb sau o poezie în limba română (folosiți fișierele proverbRo.txt sau poezieRo.txt)\n",
    "# Varianta 1 – Implementați un lanț Markov cu o singură stare sau\n",
    "\n",
    "#asociez fiecare cuvant cu urmatorul din text\n",
    "#intr-un dictionar\n",
    "def lant_markov_1stare(text):\n",
    "    words = text.split()\n",
    "    lant = {}\n",
    "    for i in range(len(words)-1):\n",
    "        word = words[i]\n",
    "        next_word = words[i+1]\n",
    "        if word not in lant.keys():\n",
    "            lant[word] = [next_word]  \n",
    "        else:\n",
    "            lant[word].append(next_word)  \n",
    "    return lant\n",
    "\n",
    "#genereaza proverbul\n",
    "def generate_1stare(lant, length):\n",
    "    proverb = []\n",
    "    word = random.choice(list(lant.keys()))\n",
    "    while word != '.' and len(proverb) < length:\n",
    "        proverb.append(word)\n",
    "        word = random.choice(lant.get(word, [\".\"]))\n",
    "    if word != '.':\n",
    "        proverb.append(word)\n",
    "    return ' '.join(proverb)\n",
    "\n",
    "file_to_Read = \"proverbRo.txt\"\n",
    "# file_to_Read = \"poezieRo.txt\"\n",
    "_text = load_data(file_to_Read)\n",
    "_lant = lant_markov_1stare(_text)\n",
    "_proverb = generate_1stare(_lant, 15)\n",
    "print(_proverb)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gata de i-ar curge. La pomul laudat sa se cauta la despartire. Adevarul se cearta al\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T08:00:37.419624Z",
     "start_time": "2024-04-22T08:00:37.124665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1b) Varianta 2 – Implementați un lanț Markov cu n-stări'\n",
    "\n",
    "def lant_markov_nstari(text, n):\n",
    "    words = text.split()\n",
    "    lant = {}\n",
    "    for i in range(len(words) - n):\n",
    "        # n - nr stari\n",
    "        state = tuple(words[i:i+n])\n",
    "        next_word = words[i+n] #asociez cu urmatorul cuvant\n",
    "        if state not in lant.keys():\n",
    "            lant[state] = [next_word]\n",
    "        else:\n",
    "            lant[state].append(next_word)\n",
    "    return lant\n",
    "\n",
    "def generate_nstari(lant, length):\n",
    "    proverb = []\n",
    "    state = random.choice(list(lant.keys())) #select random state\n",
    "    for _ in range(length):\n",
    "        next_word = random.choice(lant.get(state, [\".\"])) #cauta cuvantul. nu il gaseste, adauga .\n",
    "        if next_word == '.':\n",
    "            break\n",
    "        proverb.append(next_word) #adaug cuvantul la proverb\n",
    "        state = state[1:] + (next_word,) #update la cuvinte din starea curenta\n",
    "    return ' '.join(proverb)\n",
    "\n",
    "\n",
    "# file_to_Read = \"proverbRo.txt\"\n",
    "file_to_Read = \"poezieRo.txt\"\n",
    "_text = load_data(file_to_Read)\n",
    "n_stari = 2\n",
    "_lant = lant_markov_nstari(_text, n_stari)\n",
    "_proverb = generate_nstari(_lant, 15)\n",
    "print(_proverb)"
   ],
   "id": "5d7432a54b73fb99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "par desprinse parcă din trecut A mai trecut o toamnă după toamnă Şi frunza ce\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T08:00:48.000645Z",
     "start_time": "2024-04-22T08:00:47.932190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2a Generare în limba engleză:\n",
    "# a. Folosiți biblioteca markovify (sau implementarea voastră de la problema 1) pentru a genera o strofă de poezie în limba engleză folosind unul din următoarele corpus-uri (sau orice altă sursă găsiți voi):\n",
    "\n",
    "import markovify\n",
    "\n",
    "text = load_data(\"sonnet.txt\")\n",
    "model = markovify.Text(text) #construire model\n",
    "generare = model.make_sentence() # generare strofa\n",
    "print(generare)\n"
   ],
   "id": "ec2cbaf5c74a9972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purple pride Which on thy sport, Cannot dispraise, but in the world's false subtleties.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T07:41:17.328707Z",
     "start_time": "2024-04-22T07:41:17.322944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2b. Calculați emoția textului generat, puteți folosi una din următoarele resurse:\n",
    "# Natural Language Toolkit (nltk) SentimentIntensityAnalyzer\n",
    "# TextBlob sentiment\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# tokenizare, analiza gramaticala (ce parte de vorbire e: substantiv, etc)\n",
    "# analiza sentiment (reguli, polaritate), calcul polaritate(-1,1)\n",
    "blob = TextBlob(generare)\n",
    "feel = blob.sentiment.polarity\n",
    "print(feel)\n"
   ],
   "id": "7d10e59dac33e3d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T08:08:44.452995Z",
     "start_time": "2024-04-22T08:08:44.407772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "text = ' '.join(text)\n",
    "generare = ' '.join(generare)\n",
    "\n",
    "original_tokens = nltk.sent_tokenize(text)\n",
    "generated_tokens = nltk.sent_tokenize(generare)\n",
    "\n",
    "bleu_scores = []\n",
    "for original_sent in original_tokens:\n",
    "    max_bleu_score = max([sentence_bleu([nltk.word_tokenize(original_sent)], nltk.word_tokenize(generated_sent)) for generated_sent in generated_tokens])\n",
    "    bleu_scores.append(max_bleu_score)\n",
    "    \n",
    "bleu_score = sentence_bleu(original_tokens, generated_tokens)\n",
    "print(bleu_score)"
   ],
   "id": "af2e5a47abf08830",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fraction.__new__() got an unexpected keyword argument '_normalize'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[45], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m bleu_scores \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m original_sent \u001B[38;5;129;01min\u001B[39;00m original_tokens:\n\u001B[1;32m---> 12\u001B[0m     max_bleu_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m([\u001B[43msentence_bleu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43moriginal_sent\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated_sent\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m generated_sent \u001B[38;5;129;01min\u001B[39;00m generated_tokens])\n\u001B[0;32m     13\u001B[0m     bleu_scores\u001B[38;5;241m.\u001B[39mappend(max_bleu_score)\n\u001B[0;32m     15\u001B[0m bleu_score \u001B[38;5;241m=\u001B[39m sentence_bleu(original_tokens, generated_tokens)\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:107\u001B[0m, in \u001B[0;36msentence_bleu\u001B[1;34m(references, hypothesis, weights, smoothing_function, auto_reweigh)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentence_bleu\u001B[39m(\n\u001B[0;32m     21\u001B[0m     references,\n\u001B[0;32m     22\u001B[0m     hypothesis,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     auto_reweigh\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     26\u001B[0m ):\n\u001B[0;32m     27\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;03m    Calculate BLEU score (Bilingual Evaluation Understudy) from\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;124;03m    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03m    :rtype: float / list(float)\u001B[39;00m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcorpus_bleu\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mreferences\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mhypothesis\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msmoothing_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauto_reweigh\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:210\u001B[0m, in \u001B[0;36mcorpus_bleu\u001B[1;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m references, hypothesis \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(list_of_references, hypotheses):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;66;03m# For each order of ngram, calculate the numerator and\u001B[39;00m\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;66;03m# denominator for the corpus-level modified precision.\u001B[39;00m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, max_weight_length \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 210\u001B[0m         p_i \u001B[38;5;241m=\u001B[39m \u001B[43mmodified_precision\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreferences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhypothesis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m         p_numerators[i] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m p_i\u001B[38;5;241m.\u001B[39mnumerator\n\u001B[0;32m    212\u001B[0m         p_denominators[i] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m p_i\u001B[38;5;241m.\u001B[39mdenominator\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:368\u001B[0m, in \u001B[0;36mmodified_precision\u001B[1;34m(references, hypothesis, n)\u001B[0m\n\u001B[0;32m    364\u001B[0m \u001B[38;5;66;03m# Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\u001B[39;00m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;66;03m# Usually this happens when the ngram order is > len(reference).\u001B[39;00m\n\u001B[0;32m    366\u001B[0m denominator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28msum\u001B[39m(counts\u001B[38;5;241m.\u001B[39mvalues()))\n\u001B[1;32m--> 368\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFraction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdenominator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: Fraction.__new__() got an unexpected keyword argument '_normalize'"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (D:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# c. Pentru a adresa limitările de creativitate în poezia generată înlocuiți aleator cuvinte cu sinonime. Se cere ca sinonimele să fie obținute folosind embedding-uri. (i.e. Cuvântul ales e transformat în forma sa embedded și se alege embedding-ul cel mai apropiat care este convertit la string)\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdownloader\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mapi\u001B[39;00m\n\u001B[0;32m      5\u001B[0m word2vec_model \u001B[38;5;241m=\u001B[39m api\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword2vec-google-news-300\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m sinonime \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\gensim\\__init__.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m4.3.2\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[0;32m     14\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgensim\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m logger\u001B[38;5;241m.\u001B[39mhandlers:  \u001B[38;5;66;03m# To ensure reload() doesn't add another one\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mindexedcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m IndexedCorpus  \u001B[38;5;66;03m# noqa:F401 must appear before the other classes\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmmcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MmCorpus  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbleicorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BleiCorpus  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interfaces, utils\n\u001B[0;32m     16\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mIndexedCorpus\u001B[39;00m(interfaces\u001B[38;5;241m.\u001B[39mCorpusABC):\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\gensim\\interfaces.py:19\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m \n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils, matutils\n\u001B[0;32m     22\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCorpusABC\u001B[39;00m(utils\u001B[38;5;241m.\u001B[39mSaveLoad):\n",
      "File \u001B[1;32mD:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\gensim\\matutils.py:20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m entropy\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_blas_funcs, triu\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlapack\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_lapack_funcs\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspecial\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m psi  \u001B[38;5;66;03m# gamma function utils\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'triu' from 'scipy.linalg' (D:\\Facultate\\ubb-fmi-work\\Semestrul IV\\AI\\LAB7\\l7-rezolvari\\.venv\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "execution_count": 36,
   "source": [
    "# c. Pentru a adresa limitările de creativitate în poezia generată înlocuiți aleator cuvinte cu sinonime. Se cere ca sinonimele să fie obținute folosind embedding-uri. (i.e. Cuvântul ales e transformat în forma sa embedded și se alege embedding-ul cel mai apropiat care este convertit la string)\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "sinonime = []\n",
    "\n",
    "for word in blob.words:\n",
    "    word_vect = word2vec_model[word]\n",
    "    similar_words = word2vec_model.most_similar([word_vect])\n",
    "    sinonime.append([similar_word[0] for similar_word in similar_words])\n",
    "\n",
    "print(sinonime)\n"
   ],
   "id": "23a10ca1c0880fa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T07:00:48.497952Z",
     "start_time": "2024-04-22T07:00:48.496486Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dd38b6cb47816f10",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
